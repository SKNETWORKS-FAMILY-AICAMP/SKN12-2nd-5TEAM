{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ebbbec8-75a7-420c-a2b9-0853dbe300b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\gobok\\anaconda3\\envs\\gym-churn\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\gobok\\anaconda3\\envs\\gym-churn\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739609e7-7e1e-4332-9e99-b6f0a3b7c5b0",
   "metadata": {},
   "source": [
    "# 고객 이탈 예측 모델 개발 과정 (Machine Learning Pipeline)\n",
    "\n",
    "```\n",
    "이 문서는 이커머스 고객 이탈 예측 프로젝트의 전체 머신러닝 파이프라인을 정리한 문서입니다.  \n",
    "모델 비교 및 선정, 하이퍼파라미터 튜닝, 최종 선택 모델 선정까지의 흐름을 코드와 함께 정리합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1713c39f-e291-4b3a-ad5f-44fc165ee3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. 라이브러리 불러오기 (`from` / `import`)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import joblib\n",
    "\n",
    "# 데이터 분할 및 검증\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "# 데이터 전처리\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 평가 지표\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# 분류 모델\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    BaggingClassifier,\n",
    "    ExtraTreesClassifier\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# 부스팅 모델\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca3a133a-8a51-4df4-83b8-1bfd9bfa42ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Could not find the number of physical cores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a66d5-6ef1-4418-b178-528da766f900",
   "metadata": {},
   "source": [
    "2. 데이터 전처리\n",
    "\n",
    "```\n",
    "    결측치 처리 (mean, dropna 등)\n",
    "    범주형 인코딩 (pd.get_dummies, LabelEncoder)\n",
    "    클래스 불균형 처리 (SMOTE)\n",
    "    피처 스케일링 (StandardScaler)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f951c5-fd4b-48a4-aef9-5ffc65dfe01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. 데이터 전처리\n",
    "\n",
    "# 1. 데이터 로드\n",
    "file_path = 'E Commerce Dataset2.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df.drop(columns=['CustomerID'], inplace=True)\n",
    "\n",
    "# 3. 결측치 처리 - 수치형 평균으로 대체\n",
    "num_cols_to_impute = [\n",
    "    'Tenure', 'WarehouseToHome', 'HourSpendOnApp', 'OrderAmountHikeFromlastYear',\n",
    "    'CouponUsed', 'OrderCount', 'DaySinceLastOrder'\n",
    "]\n",
    "for col in num_cols_to_impute:\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "# 4. 이상치 제거 (IQR 기준)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols.remove('Churn')\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "    df = df[(df[col] >= lower) & (df[col] <= upper)]\n",
    "\n",
    "# 5. 타겟 분리 및 범주형 인코딩\n",
    "y = df['Churn']\n",
    "X = df.drop(columns=['Churn'])\n",
    "cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# 7. Train/Test 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 8. SMOTE 적용 (Train에만)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 9. 스케일링 (Logistic 등 안정화 목적)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1b7c88-ef93-4cd7-a17d-b1d7ea6c9b6b",
   "metadata": {},
   "source": [
    "# 3. 주요 12개 머신러닝 모델 학습 및 비교\n",
    "\n",
    "```\n",
    "Logistic Regression\n",
    "Decision Tree\n",
    "Random Forest\n",
    "Gradient Boosting\n",
    "AdaBoost\n",
    "Bagging\n",
    "ExtraTrees\n",
    "K-Nearest Neighbors\n",
    "SVC\n",
    "Naive Bayes\n",
    "XGBoost\n",
    "LightGBM\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7583cd62-8a14-4ff2-92de-43bcd4c2dcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================== LogisticRegression ==================\n",
      "Train Accuracy: 0.8579\n",
      "Test Accuracy:  0.8293\n",
      "Precision:      0.5503\n",
      "Recall:         0.6739\n",
      "F1 Score:       0.6059\n",
      "AUC:            0.8548\n",
      "CV 평균 F1: 0.8396, 표준편차: 0.0723\n",
      "⚠️ 교차검증 변동성 존재\n",
      "\n",
      "[분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.87      0.89       571\n",
      "           1       0.55      0.67      0.61       138\n",
      "\n",
      "    accuracy                           0.83       709\n",
      "   macro avg       0.73      0.77      0.75       709\n",
      "weighted avg       0.85      0.83      0.84       709\n",
      "\n",
      "\n",
      "================== DecisionTree ==================\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy:  0.8829\n",
      "Precision:      0.6730\n",
      "Recall:         0.7754\n",
      "F1 Score:       0.7205\n",
      "AUC:            0.8421\n",
      "⚠️ Train-Test 차이: 0.1171 → 과적합 가능\n",
      "CV 평균 F1: 0.9031, 표준편차: 0.0313\n",
      "\n",
      "[분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93       571\n",
      "           1       0.67      0.78      0.72       138\n",
      "\n",
      "    accuracy                           0.88       709\n",
      "   macro avg       0.81      0.84      0.82       709\n",
      "weighted avg       0.89      0.88      0.89       709\n",
      "\n",
      "\n",
      "================== RandomForest ==================\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy:  0.9394\n",
      "Precision:      0.9358\n",
      "Recall:         0.7391\n",
      "F1 Score:       0.8259\n",
      "AUC:            0.9765\n",
      "CV 평균 F1: 0.9494, 표준편차: 0.0430\n",
      "\n",
      "[분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96       571\n",
      "           1       0.94      0.74      0.83       138\n",
      "\n",
      "    accuracy                           0.94       709\n",
      "   macro avg       0.94      0.86      0.89       709\n",
      "weighted avg       0.94      0.94      0.94       709\n",
      "\n",
      "\n",
      "================== GradientBoosting ==================\n",
      "Train Accuracy: 0.9360\n",
      "Test Accuracy:  0.8942\n",
      "Precision:      0.7405\n",
      "Recall:         0.7029\n",
      "F1 Score:       0.7212\n",
      "AUC:            0.9183\n",
      "CV 평균 F1: 0.9045, 표준편차: 0.0702\n",
      "⚠️ 교차검증 변동성 존재\n",
      "\n",
      "[분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93       571\n",
      "           1       0.74      0.70      0.72       138\n",
      "\n",
      "    accuracy                           0.89       709\n",
      "   macro avg       0.83      0.82      0.83       709\n",
      "weighted avg       0.89      0.89      0.89       709\n",
      "\n",
      "\n",
      "================== AdaBoost ==================\n",
      "Train Accuracy: 0.9100\n",
      "Test Accuracy:  0.8646\n",
      "Precision:      0.6438\n",
      "Recall:         0.6812\n",
      "F1 Score:       0.6620\n",
      "AUC:            0.8946\n",
      "CV 평균 F1: 0.8842, 표준편차: 0.0898\n",
      "⚠️ 교차검증 변동성 존재\n",
      "\n",
      "[분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       571\n",
      "           1       0.64      0.68      0.66       138\n",
      "\n",
      "    accuracy                           0.86       709\n",
      "   macro avg       0.78      0.80      0.79       709\n",
      "weighted avg       0.87      0.86      0.87       709\n",
      "\n",
      "\n",
      "================== Bagging ==================\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy:  0.9365\n",
      "Precision:      0.8661\n",
      "Recall:         0.7971\n",
      "F1 Score:       0.8302\n",
      "AUC:            0.9699\n",
      "CV 평균 F1: 0.9413, 표준편차: 0.0391\n",
      "\n",
      "[분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       571\n",
      "           1       0.87      0.80      0.83       138\n",
      "\n",
      "    accuracy                           0.94       709\n",
      "   macro avg       0.91      0.88      0.90       709\n",
      "weighted avg       0.94      0.94      0.94       709\n",
      "\n",
      "\n",
      "================== ExtraTrees ==================\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy:  0.9309\n",
      "Precision:      0.9238\n",
      "Recall:         0.7029\n",
      "F1 Score:       0.7984\n",
      "AUC:            0.9798\n",
      "CV 평균 F1: 0.9465, 표준편차: 0.0436\n",
      "\n",
      "[분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96       571\n",
      "           1       0.92      0.70      0.80       138\n",
      "\n",
      "    accuracy                           0.93       709\n",
      "   macro avg       0.93      0.84      0.88       709\n",
      "weighted avg       0.93      0.93      0.93       709\n",
      "\n",
      "\n",
      "================== KNN ==================\n",
      "Train Accuracy: 0.9448\n",
      "Test Accuracy:  0.8110\n",
      "Precision:      0.5119\n",
      "Recall:         0.6232\n",
      "F1 Score:       0.5621\n",
      "AUC:            0.8426\n",
      "⚠️ Train-Test 차이: 0.1338 → 과적합 가능\n",
      "CV 평균 F1: 0.9030, 표준편차: 0.0196\n",
      "\n",
      "[분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88       571\n",
      "           1       0.51      0.62      0.56       138\n",
      "\n",
      "    accuracy                           0.81       709\n",
      "   macro avg       0.71      0.74      0.72       709\n",
      "weighted avg       0.83      0.81      0.82       709\n",
      "\n",
      "\n",
      "================== SVC ==================\n",
      "Train Accuracy: 0.9575\n",
      "Test Accuracy:  0.8942\n",
      "Precision:      0.7692\n",
      "Recall:         0.6522\n",
      "F1 Score:       0.7059\n",
      "AUC:            0.9053\n",
      "CV 평균 F1: 0.9078, 표준편차: 0.0594\n",
      "⚠️ 교차검증 변동성 존재\n",
      "\n",
      "[분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94       571\n",
      "           1       0.77      0.65      0.71       138\n",
      "\n",
      "    accuracy                           0.89       709\n",
      "   macro avg       0.84      0.80      0.82       709\n",
      "weighted avg       0.89      0.89      0.89       709\n",
      "\n",
      "\n",
      "================== GaussianNB ==================\n",
      "Train Accuracy: 0.5510\n",
      "Test Accuracy:  0.2863\n",
      "Precision:      0.2125\n",
      "Recall:         0.9855\n",
      "F1 Score:       0.3496\n",
      "AUC:            0.7291\n",
      "⚠️ Train-Test 차이: 0.2647 → 과적합 가능\n",
      "CV 평균 F1: 0.6893, 표준편차: 0.0022\n",
      "\n",
      "[분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.12      0.21       571\n",
      "           1       0.21      0.99      0.35       138\n",
      "\n",
      "    accuracy                           0.29       709\n",
      "   macro avg       0.59      0.55      0.28       709\n",
      "weighted avg       0.82      0.29      0.24       709\n",
      "\n",
      "\n",
      "================== XGBoost ==================\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy:  0.9690\n",
      "Precision:      0.9531\n",
      "Recall:         0.8841\n",
      "F1 Score:       0.9173\n",
      "AUC:            0.9771\n",
      "CV 평균 F1: 0.9600, 표준편차: 0.0432\n",
      "\n",
      "[분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       571\n",
      "           1       0.95      0.88      0.92       138\n",
      "\n",
      "    accuracy                           0.97       709\n",
      "   macro avg       0.96      0.94      0.95       709\n",
      "weighted avg       0.97      0.97      0.97       709\n",
      "\n",
      "\n",
      "================== LightGBM ==================\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2283, number of negative: 2283\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2082\n",
      "[LightGBM] [Info] Number of data points in the train set: 4566, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Train Accuracy: 0.9978\n",
      "Test Accuracy:  0.9478\n",
      "Precision:      0.9106\n",
      "Recall:         0.8116\n",
      "F1 Score:       0.8582\n",
      "AUC:            0.9800\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 1826, number of negative: 1826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2089\n",
      "[LightGBM] [Info] Number of data points in the train set: 3652, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\gobok\\anaconda3\\envs\\gym-churn\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"C:\\Users\\gobok\\anaconda3\\envs\\gym-churn\\lib\\subprocess.py\", line 503, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"C:\\Users\\gobok\\anaconda3\\envs\\gym-churn\\lib\\subprocess.py\", line 971, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\gobok\\anaconda3\\envs\\gym-churn\\lib\\subprocess.py\", line 1456, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 1827, number of negative: 1826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2080\n",
      "[LightGBM] [Info] Number of data points in the train set: 3653, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500137 -> initscore=0.000547\n",
      "[LightGBM] [Info] Start training from score 0.000547\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 1827, number of negative: 1826\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2073\n",
      "[LightGBM] [Info] Number of data points in the train set: 3653, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500137 -> initscore=0.000547\n",
      "[LightGBM] [Info] Start training from score 0.000547\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 1826, number of negative: 1827\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2076\n",
      "[LightGBM] [Info] Number of data points in the train set: 3653, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499863 -> initscore=-0.000547\n",
      "[LightGBM] [Info] Start training from score -0.000547\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 1826, number of negative: 1827\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2097\n",
      "[LightGBM] [Info] Number of data points in the train set: 3653, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499863 -> initscore=-0.000547\n",
      "[LightGBM] [Info] Start training from score -0.000547\n",
      "CV 평균 F1: 0.9523, 표준편차: 0.0422\n",
      "\n",
      "[분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       571\n",
      "           1       0.91      0.81      0.86       138\n",
      "\n",
      "    accuracy                           0.95       709\n",
      "   macro avg       0.93      0.90      0.91       709\n",
      "weighted avg       0.95      0.95      0.95       709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 3. 주요 12개 머신러닝 모델 학습 및 비교\n",
    "\n",
    "# 4. 모델 정의\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=3000, random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    \"Bagging\": BaggingClassifier(n_estimators=100, random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVC\": SVC(probability=True, random_state=42),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "# ✅ 5. 모델 학습 및 평가\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n================== {name} ==================\")\n",
    "\n",
    "    # Logistic, SVC 등 스케일링된 데이터 사용\n",
    "    if name in [\"LogisticRegression\", \"KNN\", \"SVC\", \"GaussianNB\"]:\n",
    "        model.fit(X_train_scaled, y_train_res)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, \"predict_proba\") else y_pred\n",
    "    else:\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else y_pred\n",
    "\n",
    "    train_acc = model.score(X_train_scaled if name in [\"LogisticRegression\", \"KNN\", \"SVC\", \"GaussianNB\"] else X_train_res, y_train_res)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    delta = round(train_acc - test_acc, 4)\n",
    "\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Test Accuracy:  {test_acc:.4f}\")\n",
    "    print(f\"Precision:      {precision:.4f}\")\n",
    "    print(f\"Recall:         {recall:.4f}\")\n",
    "    print(f\"F1 Score:       {f1:.4f}\")\n",
    "    print(f\"AUC:            {auc:.4f}\")\n",
    "    if delta > 0.1:\n",
    "        print(f\"⚠️ Train-Test 차이: {delta} → 과적합 가능\")\n",
    "\n",
    "    cv_f1 = cross_val_score(model,\n",
    "                            X_train_scaled if name in [\"LogisticRegression\", \"KNN\", \"SVC\", \"GaussianNB\"] else X_train_res,\n",
    "                            y_train_res, cv=5, scoring='f1')\n",
    "    print(f\"CV 평균 F1: {cv_f1.mean():.4f}, 표준편차: {cv_f1.std():.4f}\")\n",
    "    if cv_f1.std() > 0.05:\n",
    "        print(\"⚠️ 교차검증 변동성 존재\")\n",
    "\n",
    "    print(\"\\n[분류 리포트]\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b71a168-4a9d-4933-8859-5744e7277ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gobok\\anaconda3\\envs\\gym-churn\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "C:\\Users\\gobok\\anaconda3\\envs\\gym-churn\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "C:\\Users\\gobok\\anaconda3\\envs\\gym-churn\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "🎯 Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "✅ Train Accuracy: 1.0\n",
      "✅ Test Accuracy : 0.9746121297602257\n",
      "✅ Precision     : 0.9838709677419355\n",
      "✅ Recall        : 0.8840579710144928\n",
      "✅ F1 Score      : 0.9312977099236642\n",
      "✅ AUC Score     : 0.9846188989568264\n",
      "⚠️ 과적합 여부 (Train - Test F1): 0.0687\n",
      "\n",
      "[분류 리포트]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       571\n",
      "           1       0.98      0.88      0.93       138\n",
      "\n",
      "    accuracy                           0.97       709\n",
      "   macro avg       0.98      0.94      0.96       709\n",
      "weighted avg       0.97      0.97      0.97       709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 - GridSearchCV( XGBoost )\n",
    "\n",
    "# 1. 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 2. XGBoost 분류기 정의\n",
    "xgb = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3. 파라미터 후보\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# 4. GridSearchCV로 튜닝\n",
    "grid = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# 5. 최적 모델 저장\n",
    "best_model = grid.best_estimator_\n",
    "joblib.dump(best_model, \"xgb_best_model.pkl\")\n",
    "\n",
    "# 6. 평가 (Train/Test)\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "y_test_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 7. 성능 출력\n",
    "print(\"🎯 Best Parameters:\", grid.best_params_)\n",
    "print(\"✅ Train Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"✅ Test Accuracy :\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"✅ Precision     :\", precision_score(y_test, y_test_pred))\n",
    "print(\"✅ Recall        :\", recall_score(y_test, y_test_pred))\n",
    "print(\"✅ F1 Score      :\", f1_score(y_test, y_test_pred))\n",
    "print(\"✅ AUC Score     :\", roc_auc_score(y_test, y_test_proba))\n",
    "\n",
    "# 8. 과적합 판단\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "print(\"⚠️ 과적합 여부 (Train - Test F1):\", round(train_f1 - test_f1, 4))\n",
    "\n",
    "# 9. 상세 분류 리포트\n",
    "print(\"\\n[분류 리포트]\\n\", classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b0eef7-70f0-4b44-91b9-7f0946d47187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "xgb_model = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "# 학습 (SMOTE 적용된 데이터)\n",
    "xgb_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# 예측\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "y_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 평가 지표\n",
    "train_acc = xgb_model.score(X_train_res, y_train_res)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "delta = round(train_acc - test_acc, 4)\n",
    "\n",
    "# 출력\n",
    "print(\"✅ XGBoost 평가 결과\")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy : {test_acc:.4f}\")\n",
    "print(f\"Precision     : {precision:.4f}\")\n",
    "print(f\"Recall        : {recall:.4f}\")\n",
    "print(f\"F1 Score      : {f1:.4f}\")\n",
    "print(f\"AUC Score     : {auc:.4f}\")\n",
    "print(f\"⚠️ 과적합 여부 (Train - Test F1): {round(f1_score(y_train_res, xgb_model.predict(X_train_res)) - f1, 4)}\")\n",
    "\n",
    "# 교차검증\n",
    "cv_f1 = cross_val_score(xgb_model, X_train_res, y_train_res, cv=5, scoring='f1')\n",
    "print(f\"CV 평균 F1: {cv_f1.mean():.4f}, 표준편차: {cv_f1.std():.4f}\")\n",
    "if cv_f1.std() > 0.05:\n",
    "    print(\"⚠️ 교차검증 변동성 존재\")\n",
    "\n",
    "# 분류 리포트\n",
    "print(\"\\n[분류 리포트]\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 모델 저장 경로 지정\n",
    "save_path = \"./models\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# 모델 저장\n",
    "joblib.dump(best_model, os.path.join(save_path, \"xgboost_best_model.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace329fe-ebd9-49f6-99f6-3d6721317750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (churn-env)",
   "language": "python",
   "name": "churn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
